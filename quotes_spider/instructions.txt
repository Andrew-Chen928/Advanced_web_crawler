start project:
	scrapy startproject "project name" (same as django)
	
generate spiders:
	scrapy genspider "spider name" "url of the website"

using scrapy shell:
	scrapy shell (same as django)

	fetch ('https://www.upwork.com/o/jobs/browse/')
	response.xpath('//h2/a/text()').extract()
	from scrapy.selector import Selector
	sel = Selector(text=html_doc)
	sel.extract()
	sel.xpath('//p[1]').extract()
	sel.xpath('//p')[0].extract()
	// Note: select start count elements from 1, python start from 0

	sel.css('h2') // css and xpath both works for selector

	To export scraping data to a file:
	Scrapy support three different format: csv, json, xml
	simply runnnig the spider with -o "filename"

example:
	scrapy crawl quotes -o quotes_item.csv 
	scrapy crawl quotes -o quotes_item.json 
	scrapy crawl quotes -o quotes_item.xml

Notes: 
It is very important to be careful while scraping websites; otherwise, you might be banned. Here are some tips to keep in mind while web scraping:

1- In the file settings.py activate the option DOWNLOAD_DELAY or you can do that manually in your code through sleeping a for a random number of seconds.

2- In the file settings.py activate the option USER_AGENT like the following, or any Chrome or Firefox user agent here. Defining a user agent let you look more like a browser used by a real person, not an automatic robot.

USER_AGENT = "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1" 

3- Find external proxies and rotate IP addresses while scraping. You can use the package scrapy-proxies for the purpose. https://github.com/aivarsk/scrapy-proxies

4- For professional work, consider using ScrapingHub.com to host your scrapers - it offers a free limited plan.